[//]: # (Image References)

[image1]: https://user-images.githubusercontent.com/10624937/42135623-e770e354-7d12-11e8-998d-29fc74429ca2.gif "Trained Agent"

# Project 3: Collaboration and Competition

## Introduction

This project implements the **MADDPG** algorithm to control the
two agents from the **Tennis ml-agents** environment. We model the problem as a
multi-agent reinforcement learning setup and the implementation is based on the
MADDPG algorithm from the paper [**Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments**](https://arxiv.org/pdf/1706.02275.pdf)
by Lowe, et. al.. 
The trained two agents, that they pass the ball over and avoid dropping it or throwing it out of bounds (get the longest rally possible), are demonstrated below.

![Trained Agent][image1]

The environment chosen for the project is a **modified version of the Tennis Environment** 
from the Unity ML-Agents toolkit. The original version can be found [here](https://github.com/Unity-Technologies/ml-agents/blob/9b1a39982fd03de8f40f85d61f903e6d972fd2cc/docs/Learning-Environment-Examples.md#tennis),
and the version we will work with consists of a custom build provided by Udacity 
with the following descriptions:

* A group of 2 agents (colored rackets) and a ball (orange sphere). The agents try
  to keep the ball from falling and try to pass it as much as they can.

* Each agent receives an **observation** consisting of a 24-dimensional vector. This
  local observation (per agent) is generated by an internal observation of a 8-dimensional
  vector stacked 3 times along timesteps (hence 24-d).

* Each agent can move around by applying **actions** consisting of speed factors
  applied along the x and y axes, giving a 2d action vector per agent.

* Each agent gets a **reward** of +0.1 each time it passes the ball over to the other
  side of the net, and reward of -0.01 if it lets the ball fall or if it hits it out
  of bounds. The environment is (somewhat counter-intuitive) cooperative in the sense that all agents try to get
  as much reward as possible without getting in each other's way, because there is no penalty for a agent of the other agent hit the ball. This environment is considered solved once the average over 
  episodes of the maximum reward obtained among both agent is maintained above +0.5 
  over 100 episodes.

* The task is **episodic**, and it resets either by reaching the maximum number of
  steps (25000) or by the ball being drop to the floor or thrown out of bounds.

## Installations

### Downloading the environment

Download the environment from one of the links below. You need only select the environment that matches your operating system (this project uses 20 agents version of Reacher environment):

Platform | Link
-------- | -----
Linux             | [Link](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Linux.zip)
Mac OSX           | [Link](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis.app.zip)
Windows (32-bit)  | [Link](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86.zip)
Windows (64-bit)  | [Link](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Windows_x86_64.zip)

    
(_For Windows users_) Check out [this link](https://support.microsoft.com/en-us/help/827218/how-to-determine-whether-a-computer-is-running-a-32-bit-version-or-64) if you need help with determining if your computer is running a 32-bit version or 64-bit version of the Windows operating system.

(_For AWS_) If you'd like to train the agent on AWS (and have not [enabled a virtual screen](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md)), then please use [this link](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P3/Tennis/Tennis_Linux_NoVis.zip) to obtain the "headless" version of the environment.  You will **not** be able to watch the agent without enabling a virtual screen, but you will be able to train the agent.  (_To watch the agent, you should follow the instructions to [enable a virtual screen](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md), and then download the environment for the **Linux** operating system above._)

Unzip (or decompress) the downloaded file and store the path of the executable as we will need the path to input on `Tennis.ipynb`. 

Note that this environment is compatible only with an older version of the ML-Agents toolkit (version 0.4.0), the next setup section will take care of this.

### Resolving dependencies

Please follow the instructions on [DRLND Github repository](https://github.com/udacity/deep-reinforcement-learning#dependencies) to install the required libraries.

Additionally, please also install `torchsummary` to visualize the model descriptions. Open your Anaconda environment that you just create (`dlrnd`) and type:

```bash
pip install torchsummary
```

## Usages

Follow the instructions in `Tennis.ipynb` to get started with training your own agent or load the trained model and visualize the agents tracking the green objects!

## References

These are some code-references and papers I used while implementing my DDPG agent:

* [*Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments* by Lowe et. al.](https://arxiv.org/pdf/1706.02275.pdf)
* [*Continuous control through deep reinforcement learning* paper by Lillicrap et. al.](https://arxiv.org/pdf/1509.02971.pdf)
* [*Deterministic Policy Gradients Algorithms* paper by Silver et. al.](http://proceedings.mlr.press/v32/silver14.pdf)
* [DDPG implementation from Udacity](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum)