[//]: # (References)

<!-- IMAGES -->
[img_training_result]: results/train_score.png
[img_testing_result]:  results/test_score.png

# Collaboration agents playing Tennis using MADDPG model

This report consists of the following headlines: 

* [Model architecture](#1-model-architecture)
    
* [Implementation](#2-implementation)
    * [Implementation details](#21-implementation-details)
    * [Hyperparameters](#22-hyperparameters)

* [Results](#3-results)

* [Future work](#4-future-work)

Readers are recommended to read the DDPG[1], DPG[2] algorithm and then the MADDPG[6] algorithm papers to understand the theoretical background before diving into the project, it presents the theoretical foundations of the multi-agent environment settings, the actor and critic models, update rules, tweaks such as experience replay, target networks, soft updates, etc.

## 1. Model architecture

In this section, a brief description deep models that represents the actor and critic in multi-agent setting are described. The model descriptions are generated by `torchsummary` as below.

```
Actor model:

----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
       BatchNorm1d-1                   [-1, 24]              48
            Linear-2                  [-1, 256]           6,400
       BatchNorm1d-3                  [-1, 256]             512
            Linear-4                  [-1, 128]          32,896
       BatchNorm1d-5                  [-1, 128]             256
            Linear-6                    [-1, 2]             258
================================================================
Total params: 40,370
Trainable params: 40,370
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.15
Estimated Total Size (MB): 0.16
----------------------------------------------------------------
MAFCPolicy(
  (bn_input): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=24, out_features=256, bias=True)
  (bn_fc1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc2): Linear(in_features=256, out_features=128, bias=True)
  (bn_fc2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc3): Linear(in_features=128, out_features=2, bias=True)
)
----------------------------------------------------------------
----------------------------------------------------------------

Critic model:
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
       BatchNorm1d-1                   [-1, 48]              96
            Linear-2                  [-1, 128]           6,272
            Linear-3                  [-1, 128]          17,024
            Linear-4                    [-1, 1]             129
================================================================
Total params: 23,521
Trainable params: 23,521
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.09
Estimated Total Size (MB): 0.09
----------------------------------------------------------------
MAFCCritic(
  (bn_input): BatchNorm1d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=48, out_features=128, bias=True)
  (fc2): Linear(in_features=132, out_features=128, bias=True)
  (fc3): Linear(in_features=128, out_features=1, bias=True)
)
```

This choice of the model setting is the same as the actor and critic networks with batch normalization in `\continuous_control` project[7]. However, in the critic model, the input interface of the `MAFCCritic.forward()` function is as follow:

![image](https://user-images.githubusercontent.com/18066876/79847804-849c0b80-83c0-11ea-8320-dc8391fbebc2.png)

For each agent, its Q-function captures all agents' observations and actions according to [6]. Intuitively, because the problem with multi-agent setting is that the environment does not have Markov property in one agent perspective if it just considers its local observations in training phase, it is due to the non-stationary behaviors of other agents. Hence, to gain back the Markov property, the Q-function of each agent must consider all observations (and actions) and then judge how good the current observation-action pair of its agent. To be more concrete, there are two agents in this case, for the input layer has 2*24 units, and the actions is concatenate with the output of the first hidden layer, hence 128 + 2*2=132.

All of the hidden layers in both network are activated by ReLU. In the actor model, the output layer is activated with `tanh` function to limit the output range to [-1, 1], which is compatible with the environment action space.

## 2. Implementation

The project implementation considerations are presented in this section. For code-reusing purpose, the implementation of data structures, deep models and agents are placed in the folder `\agents` as a python package. The package is designed to be minimal, no generic programming method is considered.

### 2.1 Implementation details 

The whole implementation of the agent solver is packed into `MADDPGAgent` class in `agents.py`. We will now explain each components of the class.

#### Multi-agent Experience replay

This mechanism can be described as **learning from past stored experiences during replay sessions**[3]. Basically, the agent experiences are stored in memory (called a replay buffer) and learn from them later. This allows more efficient use of past experiences by not throwing away samples right away, and it also helps to break one type of correlations: sequential correlations between experiences tuples. 

Intuitionly, sequentially updating experiences introduces more bias as all real-time processes consist of sequential causal time step, we therfore effectively make the model focusing learning on current time period data and hence the bias. 

This multi-agent experience replay is different from previous experience replay[7] in the way that it stores a transition consisted of all member agent transition stacked together, hence the shape of each transition (NUM_AGENTS, {STATE|REWARD|ACTION}_SIZE). It is implemented as `MAReplayBuffer` class in `utils.py`. We uniformly sample the experiences in the `MAReplayBuffer.sample()` function, the returned transition batch will have the shape (BATCH_SIZE, NUM_AGENTS, {STATE|REWARD|ACTION}_SIZE ). There is also an `discrete_action=False|True` option to specify the action type to be continuous or discrete. Further implementation of performance tweak such as [Prioritize experience replay](https://arxiv.org/abs/1511.05952) can be implemented here.

#### Normal noise

In Tennis environment, the episodes initially have short lenght when the agents start training (the rackets cannot hit the balls then the episode ends), so Ornsteinâ€“Uhlenbeck noise model[7] has no effect to capture the random dynamic due to short amount of sequential states, and there may not be sufficient exploration due to correlated noise. Hence, Normal noise is good enough to simulate exploration behaviors of the agent around the current state. The Normal noise is simply implement as below:

```python
class NormalNoise():
    """A simple normal-noise sampler
        
    Args:
        size (tuple): size of the noise to be generated
        seed (int): random seed for the rnd-generator
        sigma (float): standard deviation of the normal distribution
    """
    def __init__(self, shape, seed, sigma = 0.2):
        self.shape = shape
        self.seed  = np.random.seed(seed)
        self.sigma = sigma

    def reset(self):
        pass

    def sample(self):
        res = self.sigma * np.random.randn(*self.shape)
        return res
```

#### Initialization

Each hidden layer parameters in both network are initialized by LeCun uniform initializer by the following function:

```python
def hidden_init(layer):
    fan_in = layer.weight.data.size()[0]
    lim = 1. / np.sqrt(fan_in)
    return (-lim, lim)
```

Except for the output layers of each network are uniformly initialized in the range of `[-3e-3, 3e-3]`.

#### MADDPG agent

With the above considerations, the MADDPG agent is implemented in `agents.py` as `MADDPGAgent` class. Some considerations are made:

- The init parameter `num_agents`: to specify the number of agents to in the collaboration or competition environment.
- Normal noise: the `NormalNoise` class is initialized in `MADDPGAgent.noises` to sample noise adding to agent action for exploration behavior.
- Experience replay: the `MAReplayBuffer` class is initialized in `MADDPGAgent.memory` to store collected transitions and uniformly sample a batch for learning. 
- Local and Target network: the class consists of `MADDPGAgent.{actor|critic}_locals` networks for training and `MADDPGAgent.{actor|critic}_targets` networks for storing delayed weights of the local networks, target networks effectively make the TD updates stable. 
- Learning routine: is implemented in `MADDPGAgent.learn()`, which strictly follows the MADDPG pseudo-algorithm described in [6].
- Gradient clipping: is used in critic model training for stability. As an example: `torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1)`
- Soft updates: instead of abrupt weight updates for target network every certain training step, we do a soft update mechanism implemented in `MADDPGAgent.soft_update()` function. This can stabilize training process further. The rule is described below:

![image](https://user-images.githubusercontent.com/18066876/79749721-fcf6c400-830f-11ea-9bb1-5295231b73c5.png)


#### Training routine

The training routine is described as follow:

```python
agent = MADDPGAgent(state_size, action_size, random_seed=0, num_agents=num_agents)

def train_maddpg(n_episodes=2000, max_t=1000, start_episode=1, print_every=100, save_every=500, saving=False, eps=1.0, eps_linear_decay=5e-6, eps_min=0.1):
    """MADDPG 
    
    Params
    ======
        n_episodes (int): maximum number of training episodes
        max_t (int): maximum number of timesteps per episode
    """
    scores = []                        # list containing scores from each episode
    scores_window = deque(maxlen=100)  # last 100 scores
    saved = False

    for i_episode in range(start_episode, start_episode+n_episodes+1):
        env_info = env.reset(train_mode=True)[brain_name]
        agent.reset()
        
        states = env_info.vector_observations
        current_scores = np.zeros(num_agents)
        for t in range(max_t):
            actions = agent.act(states, eps)
            env_info = env.step(actions)[brain_name]
            
            next_states = env_info.vector_observations  
            rewards = env_info.rewards                
            dones = env_info.local_done
            current_scores += env_info.rewards
            
            agent.step(states, actions, rewards, next_states, dones)
                
            states = next_states
            eps = max(eps_min, eps - eps_linear_decay)
            if np.any(dones):
                break 
            
        max_current_score = np.max(current_scores)
        scores_window.append(max_current_score)       # save most recent score
        scores.append(max_current_score)              # save most recent score

        mean_score = np.mean(scores_window)
        if i_episode % print_every == 0:
            print('\rEpisode {}\tAverage Score: {:.2f}\tScore: {:.2f}'.format(i_episode, mean_score, max_current_score))
        if mean_score >= 0.6 and saving and not saved:
            print('\nEnvironment solved in {:d} episodes!\tAverage Score: {:.2f}, saving models!'.format(i_episode, mean_score))
            for i in range(num_agents):
                torch.save(agent.actor_locals[i].state_dict(), 'actor{}.pth'.format(i))
                torch.save(agent.critic_locals[i].state_dict(), 'critic{}.pth'.format(i))
            saved = True
    return scores
```

For each episode, the environment are reset and the agent starts interacting witht the evironment and records the transitions via the `MADDPGAgent` class API. The `eps` noise scale is also annealed by `eps - eps_linear_decay` for each episode. For turning off the `eps` noise scale annealing, we can just set `actions = agent.act(states, eps=1.0)`. For each episode, a maximum score between agents is recorded. A running score window of lenght 100 is tracked, if the window mean is exceed 0.5, it signals the agents have successfully learnt to hit the ball with their rackets and saved the model parameters. The training function will return recorded scores for plotting purpose.

### 2.2 Hyperparameters

Please refer to `agents.py` to see how the hyperparameters are used in each part of the `MADDPGAgent` implementation. The hyperparameters are chosen as follows:

```python
MA_BUFFER_SIZE = int(1e6)  # replay buffer size
MA_BATCH_SIZE = 256        # minibatch size
MA_GAMMA = 0.999           # discount factor
MA_TAU = 1e-3              # for soft update of target parameters
MA_LR_ACTOR = 4e-4         # learning rate of the actor 
MA_LR_CRITIC = 8e-4        # learning rate of the critic
MA_EPSILON_DECAY = 5e-6    # decay factor for e-greedy linear schedule
MA_WEIGHT_DECAY = 0.0      # L2 weight decay
MA_UPDATE_EVERY = 1        # how often to update the network
MA_NUM_UPDATES  = 1        # number of updates
```

The hyperparameters are self-described. 

## 3. Results

This section presents the result of training and testing our MADDPG agent in Tennis environment.

#### Running the test

To start evaluating the agent performance, please follow the instruction on `Tennis.ipynb` section 4. 

#### Training scores

As can be seen, the agents start capturing the environment characteristic after 1800 episodes. The reason is that the good reward signals are very sparse and the episode lengths are so short at the beginning, the agents have to take the actions randomly for a long time before gathering enough good transitions. This can be harmful for the learning process if the learning rate of actor and critic model are large enough or the batch normalizaton method is not use, the agents' gradients will be sensitive to hamrful signal magnitude and may catapult the model parameters to unrecoverable region. Hence, the training process makes use of gradient clipping, batch normalization and `eps` noise scale annealing for the robustness.   

![Training score][img_training_result]

#### Testing scores

To evaluate if the MADDPG agent is succesfully trained for both model settings, this section provides testing score averaged over 100 episodes, of which no learning process happens. 
The test is run for 200 episodes. As can be seen, the mean of testing score window is maintained at around 1.0 over 100 episodes, and hence proves that the agents have learnt the Tennis environment.

![Testing score][img_testing_result] 


## 4. Future works

Finally, some of the improvements we can consider making in the following updates to this project:

* Investigate the effects of initilization in the training dynamics, as sometime the agent does not learn or capture environment characteristics.

* Implement other Policy Gradient algorithms to solve this task, like PPO and SAC, and compare them
  with our DDPG implementation as baseline.

* Run the training for longer to see if the algorithm crashes as the baseline does.

* Try to solve the optional Soccer Task (soccer env. from ml-agents)

## References

* [1] Lillicrap et. al.. [*Continuous control through deep reinforcement learning*](https://arxiv.org/pdf/1509.02971.pdf)
* [2] Silver et. al.. [*Deterministic Policy Gradients Algorithms*](http://proceedings.mlr.press/v32/silver14.pdf)
* [3] Mnih, Volodymyr & Kavukcuoglu, Koray & Silver, David, et. al.. [*Human-level control through deep-reinforcement learning*](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)
* [4] [DDPG implementation from Udacity](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum)
* [5] [Gaussian noise](https://en.wikipedia.org/wiki/Gaussian_noise)
* [6] Lowe et. al.. [*Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments*](https://arxiv.org/pdf/1706.02275.pdf)
* [7] [Continuous Control project](../continuous_control/REPORT.md)