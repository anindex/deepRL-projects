[//]: # (References)

<!-- IMAGES -->
[img_training_result]: results/train_score.png
[img_testing_result]: results/test_score.png

# Navigation agent using DQN model

This report consists of the following headlines: 

* [Model architecture](#1-model-architecture)
    
* [Implementation](#2-implementation)
    * [Implementation details](#21-implementation-details)
    * [Hyperparameters](#22-hyperparameters)

* [Results](#3-results)

* [Future work](#4-future-work)

Readers are recommended to read the Deep Q-learning algorithm [1] paper before diving into the project, it presents the theoretical foundations of the models, update rules, tweaks such as experience replay, target networks, soft updates, etc.

## 1. Model architecture

In this section, a brief description deep model that represents the Q function is described. The model description is generated by `torchsummary` as below.

```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                   [-1, 64]           2,432
            Linear-2                   [-1, 64]           4,160
            Linear-3                    [-1, 4]             260
================================================================
Total params: 6,852
Trainable params: 6,852
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.03
Estimated Total Size (MB): 0.03
----------------------------------------------------------------
FCQNetwork(
  (fc1): Linear(in_features=37, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=4, bias=True)
)
```

As the observation shape of `Banana` environment has only 37 variables compared to visual 3x84x84 image observation of `VisualBanana` environment, a fully connected deep model is chosen, which has enough capacity to capture the complexity of Q-function. In this project, the same parameters are set according to Udacity Q-network implementation [2], which consists of 64 units for both hidden layers, no batch normalization method is used and each hidden layer is activated by ReLU.

## 2. Implementation

The project implementation considerations are presented in this section. For code-reusing purpose, the implementation of data structures, deep models and agents are placed in the folder `\agents` as a python package. The package is designed to be minimal, no generic programming method is considered.

### 2.1 Implementation details 

The whole implementation of the agent solver is packed into `DQNAgent` class in `agents.py`. We will now explain each components of the class.

#### Experience replay

This mechanism can be described as **learning from past stored experiences during replay sessions**[1]. Basically, the agent experiences are stored in memory (called a replay buffer) and learn from them later. This allows more efficient use of past experiences by not throwing away samples right away, and it also helps to break one type of correlations: sequential correlations between experiences tuples. 

Intuitionly, sequentially updating experiences introduces more bias as all real-time processes consist of sequential causal time step, we therfore effectively make the model focusing learning on current time period data and hence the bias. 

The experience replay idea is implemented as `ReplayBuffer` class in `utils.py`. We uniformly sample the experiences in the `ReplayBuffer.sample()` function, and we also have an `discrete_action=False|True` option to specify the action type to be continuous or discrete. Further implementation of performance tweak such as [Prioritize experience replay](https://arxiv.org/abs/1511.05952) can be implemented here.

#### Q-function approximation as deep models

With the expressive power of `Pytorch`, the chosen Q-network is simply implemented as a derived network from `nn.Module` as below:

```python
class FCQNetwork(nn.Module):
    """Fully connected DNN Q function which outputs array of action values"""

    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):
        """Initialize parameters and build model.
        Params
        ======
            state_size (int): Dimension of each state
            action_size (int): Dimension of each action
            seed (int): Random seed
            fc1_units (int): Number of nodes in first hidden layer
            fc2_units (int): Number of nodes in second hidden layer
        """
        super(FCQNetwork, self).__init__()
        self.seed = torch.manual_seed(seed)

        self.fc1  = nn.Linear(state_size, fc1_units)
        self.fc2  = nn.Linear(fc1_units, fc2_units) 
        self.fc3  = nn.Linear(fc2_units, action_size)

    def forward(self, state):
        """Build a network that maps state -> action values."""
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        return self.fc3(x)
```

No custom initialization is considered in this Q-network, the [default initialization](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/linear.py) for each `Linear` is used.

#### DQN agent

With the above implementations, the DQN agent is described in `agents.py` as `DQNAgent` class. Some considerations are made:

- Experience replay: the `ReplayBuffer` class is initialized in `DQNAgent.memory` to store collected transitions and uniformly sample a batch for learning. 
- Local and Target network: the class consists of `DQNAgent.qnetwork_local` network for training and `DQNAgent.qnetwork_target` network for storing delayed weights of the local network, target network effectively makes the TD updates stable. 
- Learning routine: is implemented in `DQNAgent.learn()`, which strictly follows the DDPG pseudo-algorithm described in [1].
- Soft updates: instead of abrupt weight updates for target network every certain training step, we do a soft update mechanism implemented in `DQNAgent.soft_update()` function. This can stabilize training process further. The rule is described below:

![image](https://user-images.githubusercontent.com/18066876/79749721-fcf6c400-830f-11ea-9bb1-5295231b73c5.png)


#### Training routine

The training routine is described as follow:

```python
agent = DQNAgent(state_size, action_size, seed=0)

def train_dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995, print_every=100, save_every=500, saving=False):
    """Deep Q-Learning.
    
    Params
    ======
        n_episodes (int): maximum number of training episodes
        max_t (int): maximum number of timesteps per episode
        eps_start (float): starting value of epsilon, for epsilon-greedy action selection
        eps_end (float): minimum value of epsilon
        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon
    """
    scores = []                        # list containing scores from each episode
    scores_window = deque(maxlen=100)  # last 100 scores
    eps = eps_start                    # initialize epsilon

    saved = False

    for i_episode in range(1, n_episodes+1):
        env_info = env.reset(train_mode=True)[brain_name]
        state = env_info.vector_observations[0]
        score = 0
        for t in range(max_t):
            action = agent.act(state, eps)
            env_info = env.step(action.astype(np.int32))[brain_name]
            
            next_state = env_info.vector_observations[0]   # get the next state
            reward = env_info.rewards[0]                   # get the reward
            done = env_info.local_done[0] 
            
            agent.step(state, action, reward, next_state, done)
            state = next_state
            score += reward
            if done:
                break 
        scores_window.append(score)       # save most recent score
        scores.append(score)              # save most recent score
        eps = max(eps_end, eps_decay*eps) # decrease epsilon

        mean_score = np.mean(scores_window)
        if i_episode % print_every == 0:
            print('\rEpisode {}\tAverage Score: {:.2f}'.format(i_episode, mean_score))
        if mean_score >= 13.0 and saving and not saved:
            print('\nEnvironment solved in {:d} episodes!\tAverage Score: {:.2f}'.format(i_episode-100, mean_score))
            torch.save(agent.qnetwork_local.state_dict(), 'lowdim.pth')
            saved = True
            break
    return scores
```

For each episode, the environment is reset and the agent starts interacting witht the evironment and records the transitions via the `DQNAgent` class API. The `eps` noise scale is also annealed by `eps_decay` for each episode. A running score window of lenght 100 is tracked, if the window mean is exceed 13, it signals the agent has successfully learnt to collect the yellow bananas and saved the model parameters. The training function will return recorded scores for plotting purpose.

### 2.2 Hyperparameters

Please refer to `agents.py` to see how the hyperparameters are used in each part of the `DQNAgent` implementation. The hyperparameters are chosen as follows:

```python
DQN_BUFFER_SIZE = int(1e5)  # replay buffer size
DQN_BATCH_SIZE = 64         # minibatch size
DQN_GAMMA = 0.99            # discount factor
DQN_TAU = 1e-3              # for soft update of target parameters
DQN_LR = 5e-4               # learning rate 
DQN_UPDATE_EVERY = 4        # how often to update the network
```

The hyperparameters are self-described. Note that the hyperparameters are the same as [2].

## 3. Results

This section presents the result of training and testing our DQN agent. 

#### Running the test
To start evaluating the agent performance, please follow the instruction on `Navigation.ipynb` section 4. 

#### Training scores

As we can see, the environment is solved in 700 episodes with average score of 13, but the agent is trained longer upto 2000 episodes to see the saturated maximum training score. The training scores seems noise in this environment, I think adding gradient clipping and some bacth normalization layer to make training gradient more conservaive can stabilize the training curve. The result is shown as follow:

![Training score][img_training_result]

#### Testing scores

To evaluate if my agent is succesfully trained, this section provides testing score averaged over 100 episodes, of which no learning process happens. Due to stochasticity of the environment, sometime we can see many blue bananas surrounding the agent, it then oscillates in these situations. Overall, the mean testing score reaches the target of nearly 13 (12.51) over 100 episodes to solve this environment.

![Testing score][img_testing_result]

## 4. Future works

Finally, some of the improvements we can consider making in the following updates to this project:

- Test DDPG agent in this environment to see the learning performance according to total episodes to solve the environment and maximum training score reached.

- Test the visual-based agent working for the `VisualBanana` environment, using the `CNNQNetwork` as function approximator for our Q-function (we are now dealing with image observation).

- Implement the remaining improvements from rainbow, namely Dueling DQN, Noisy DQN, A3C, Distributional DQN and compare the learning performance with the current vanilla DQN implementation.

## References

* [1] Mnih, Volodymyr & Kavukcuoglu, Koray & Silver, David, et. al.. [*Human-level control through deep-reinforcement learning*](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)
* [2] [DQN lunar-lander implementation from Udacity](https://github.com/udacity/deep-reinforcement-learning/tree/master/dqn/solution)