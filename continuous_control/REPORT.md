[//]: # (References)

<!-- IMAGES -->
[img_training_result_v1]: results/train_score_v1.png
[img_testing_result_v1]:  results/test_score_v1.png
[img_testing_result]:     results/test_score.png

# Continuous control agent using DDPG model

This report consists of the following headlines: 

* [Model architecture](#1-model-architecture)
    
* [Implementation](#2-implementation)
    * [Implementation details](#21-implementation-details)
    * [Hyperparameters](#22-hyperparameters)

* [Results](#3-results)

* [Future work](#4-future-work)

Readers are recommended to read the DDPG[1] and DPG[2] algorithm papers to understand the theoretical background before diving into the project, it presents the theoretical foundations of the actor and critic models, update rules, tweaks such as experience replay, target networks, soft updates, etc. In this project, two attempts of training the agent, each with different model settings, are presented in `Continuous_Control.ipynb` and `Continuous_Control_old.ipynb` notebook respectively.

## 1. Model architecture

In this section, a brief description deep models that represents the actor and critic are described. The model descriptions are generated by `torchsummary` as below.

### Without batch normalization

```
Actor model:

----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 256]           8,704
            Linear-2                    [-1, 4]           1,028
================================================================
Total params: 9,732
Trainable params: 9,732
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.04
Estimated Total Size (MB): 0.04
----------------------------------------------------------------
FCPolicy(
  (fc1): Linear(in_features=33, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=4, bias=True)
)
----------------------------------------------------------------
----------------------------------------------------------------

Critic model:
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                  [-1, 256]           8,704
            Linear-2                  [-1, 256]          66,816
            Linear-3                  [-1, 128]          32,896
            Linear-4                    [-1, 1]             129
================================================================
Total params: 108,545
Trainable params: 108,545
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.41
Estimated Total Size (MB): 0.42
----------------------------------------------------------------
FCCritic(
  (fc1): Linear(in_features=33, out_features=256, bias=True)
  (fc2): Linear(in_features=260, out_features=256, bias=True)
  (fc3): Linear(in_features=256, out_features=128, bias=True)
  (fc4): Linear(in_features=128, out_features=1, bias=True)
)
```

This choice of the model setting is the same as Udacity DPPG implementation[4]. However, in the critic model, the hidden layers are activated by leaky ReLU. The reason is to reduce the number of "dying units" and hence speed up the training process in term of reaching saturated training score. In the actor model, the output layer is activated with `tanh` function to limit the output range to [-1, 1], which is compatible with the environment action space.

### With batch normalization

```
Actor model:

----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
       BatchNorm1d-1                   [-1, 33]              66
            Linear-2                  [-1, 256]           8,704
       BatchNorm1d-3                  [-1, 256]             512
            Linear-4                  [-1, 128]          32,896
       BatchNorm1d-5                  [-1, 128]             256
            Linear-6                    [-1, 4]             516
================================================================
Total params: 42,950
Trainable params: 42,950
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.01
Params size (MB): 0.16
Estimated Total Size (MB): 0.17
----------------------------------------------------------------
MAFCPolicy(
  (bn_input): BatchNorm1d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=33, out_features=256, bias=True)
  (bn_fc1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc2): Linear(in_features=256, out_features=128, bias=True)
  (bn_fc2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc3): Linear(in_features=128, out_features=4, bias=True)
)
----------------------------------------------------------------
----------------------------------------------------------------

Critic model:
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
       BatchNorm1d-1                   [-1, 33]              66
            Linear-2                  [-1, 128]           4,352
            Linear-3                  [-1, 128]          17,024
            Linear-4                    [-1, 1]             129
================================================================
Total params: 21,571
Trainable params: 21,571
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.08
Estimated Total Size (MB): 0.09
----------------------------------------------------------------
MAFCCritic(
  (bn_input): BatchNorm1d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=33, out_features=128, bias=True)
  (fc2): Linear(in_features=132, out_features=128, bias=True)
  (fc3): Linear(in_features=128, out_features=1, bias=True)
)
```

This choice of the model setting has more capacity than the previous setting, because the previous training score shown in section 3 saturated at 28. This model setting adds a batch normalization layer for each hidden layer in actor model and a batch normalization layer for input layer of critic model. The reason is to make the actor model and critic model more immune to linear magnitude signal, hence making the training more conservative and stable. In the critic model, the hidden layers are now activated by ReLU. The output layer of actor model is also activated with `tanh` function to be compatible with the environment action space.

## 2. Implementation

The project implementation considerations are presented in this section. For code-reusing purpose, the implementation of data structures, deep models and agents are placed in the folder `\agents` as a python package. The package is designed to be minimal, no generic programming method is considered.

### 2.1 Implementation details 

The whole implementation of the agent solver is packed into `DDPGAgent` class in `agents.py`. We will now explain each components of the class.

#### Experience replay

This mechanism can be described as **learning from past stored experiences during replay sessions**[3]. Basically, the agent experiences are stored in memory (called a replay buffer) and learn from them later. This allows more efficient use of past experiences by not throwing away samples right away, and it also helps to break one type of correlations: sequential correlations between experiences tuples. 

Intuitionly, sequentially updating experiences introduces more bias as all real-time processes consist of sequential causal time step, we therfore effectively make the model focusing learning on current time period data and hence the bias. 

The experience replay idea is implemented as `ReplayBuffer` class in `utils.py`. We uniformly sample the experiences in the `ReplayBuffer.sample()` function, and we also have an `discrete_action=False|True` option to specify the action type to be continuous or discrete. Further implementation of performance tweak such as [Prioritize experience replay](https://arxiv.org/abs/1511.05952) can be implemented here.

#### Ornsteinâ€“Uhlenbeck noise model (OU noise)

The idea of this noise is to model for the velocity of a massive Brownian particle under the influence of friction, also called a Damped Random Walk[5]. Intuitionly, this noise model is suitable to model correlated noise in robotics control, since each time step of the robot dynamic is correlated with previous and future time steps. The implementation is presented as follow:

```python
class OUNoise:
    """Ornstein-Uhlenbeck process."""

    def __init__(self, shape, seed, mu=0., theta=0.15, sigma=0.2):
        """Initialize parameters and noise process."""
        self.mu = mu * np.ones(shape)
        self.theta = theta
        self.sigma = sigma
        self.seed = random.seed(seed)
        self.reset()

    def reset(self):
        """Reset the internal state (= noise) to mean (mu)."""
        self.state = copy.copy(self.mu)

    def sample(self):
        """Update internal state and return it as a noise sample."""
        x = self.state
        dx = self.theta * (self.mu - x) + self.sigma * np.random.rand(*x.shape) 
        self.state = x + dx
        return self.state
```

Note that this noise is state dependent, hence we must reset the noise for each training episode.

#### Initialization

Each hidden layer parameters in both network are initialized by LeCun uniform initializer by the following function:

```python
def hidden_init(layer):
    fan_in = layer.weight.data.size()[0]
    lim = 1. / np.sqrt(fan_in)
    return (-lim, lim)
```

Except for the output layers of each network are uniformly initialized in the range of `[-3e-3, 3e-3]`.

#### DDPG agent

With the above considerations, the () DDPG agent is implemented in `agents.py` as `DDPGAgent` class. Some considerations are made:

- The init parameter `batch_norm=True|False`: to choose model with batch normalization and higher capacity or not. 
- The init parameter `num_agents`: to sepcify the number of parallel agents to collect transitions.
- OU noise: the `OUNoise` class is initialized in `DDPGAgent.noises` to sample noise adding to agent action for exploration behavior.
- Experience replay: the `ReplayBuffer` class is initialized in `DDPGAgent.memory` to store collected transitions and uniformly sample a batch for learning. 
- Local and Target network: the class consists of `DDPGAgent.{actor|critic}_local` network for training and `DDPGAgent.{actor|critic}_target` network for storing delayed weights of the local network, target network effectively makes the TD updates stable. 
- Learning routine: is implemented in `DDPGAgent.learn()`, which strictly follows the DDPG pseudo-algorithm described in [1].
- Soft updates: instead of abrupt weight updates for target network every certain training step, we do a soft update mechanism implemented in `DDPGAgent.soft_update()` function. This can stabilize training process further. The rule is described below:

![image](https://user-images.githubusercontent.com/18066876/79749721-fcf6c400-830f-11ea-9bb1-5295231b73c5.png)


#### Training routine

The training routine is described as follow:

```python
agent = DDPGAgent(state_size, action_size, random_seed=0, num_agents=num_agents, batch_norm=True|False)

def train_ddpg(n_episodes=2000, max_t=1000, start_episode=1, print_every=100, save_every=500, saving=False, eps = 1.0, eps_linear_decay=1e-5, eps_min=0.1):
    """Distributed DDPG (D3PG)
    
    Params
    ======
        n_episodes (int): maximum number of training episodes
        max_t (int): maximum number of timesteps per episode
    """
    scores = []                        # list containing scores from each episode
    scores_window = deque(maxlen=100)  # last 100 scores
    
    saved = False

    for i_episode in range(start_episode, start_episode+n_episodes+1):
        env_info = env.reset(train_mode=True)[brain_name]
        agent.reset()
        
        states = env_info.vector_observations
        current_scores = np.zeros(num_agents)
        for t in range(max_t):
            actions = agent.act(states, eps)
            env_info = env.step(actions)[brain_name]
            
            next_states = env_info.vector_observations  
            rewards = env_info.rewards                
            dones = env_info.local_done
            current_scores += env_info.rewards
            
            for i in range(num_agents):
                agent.step(states[i], actions[i], rewards[i], next_states[i], dones[i])
                
            states = next_states
            eps = max(eps_min, eps - eps_linear_decay)
            if np.any(dones):
                break 
            
        avg_score = np.mean(current_scores)
        scores_window.append(avg_score)       # save most recent score
        scores.append(avg_score)              # save most recent score

        mean_score = np.mean(scores_window)
        if i_episode % print_every == 0:
            print('\rEpisode {}\tAverage Score: {:.2f}\tScore: {:.2f}'.format(i_episode, mean_score, avg_score))
        if mean_score >= 30.0 and saving and not saved:
            print('\nEnvironment solved in {:d} episodes!\tAverage Score: {:.2f}, saving models!'.format(i_episode, mean_score))
            torch.save(agent.actor_local.state_dict(), 'actor.pth')
            torch.save(agent.critic_local.state_dict(), 'critic.pth')
            saved = True
    return scores
```

For each episode, the environment and noise model are reset and the agent starts interacting witht the evironment and records the transitions via the `DDPGAgent` class API. The `eps` noise scale is also annealed by `eps - eps_linear_decay` for each episode. For turning off the `eps` noise scale annealing, we can just set `actions = agent.act(states, eps=1.0)`. A running score window of lenght 100 is tracked, if the window mean is exceed 30, it signals the agent has successfully learnt to track the green objects and saved the model parameters. The training function will return recorded scores for plotting purpose.

### 2.2 Hyperparameters

Please refer to `agents.py` to see how the hyperparameters are used in each part of the `DDPGAgent` implementation. The hyperparameters are chosen as follows:

```python
BUFFER_SIZE = int(1e6)  # replay buffer size
BATCH_SIZE = 256        # minibatch size
GAMMA = 0.99            # discount factor
TAU = 1e-3              # for soft update of target parameters
LR_ACTOR = 1e-3         # learning rate of the actor 
LR_CRITIC = 1e-3        # learning rate of the critic
WEIGHT_DECAY = 0.0      # L2 weight decay
NOISE_OU_MU = 0.0       # param mu for OU noise
NOISE_OU_THETA = 0.15   # param theta for OU noise
NOISE_OU_SIGMA = 0.2    # param sigma for OU noise
UPDATE_EVERY = 20       # how often to update the network
NUM_UPDATES  = 10       # number of updates
```

The hyperparameters are self-described. Note that each model setting training uses the same hyperparameters.

## 3. Results

This section presents the result of training and testing our DDPG agent for two model settings.

#### Running the test
To start evaluating the agent performance for each model setting, please follow the instruction on `Continuous_Control.ipynb` and `Continuous_Control_old.ipynb`section 4. 

#### Training scores

**Without batch normalization, gradient clipping and eps noise annealing**

As we can see, the mean of training score window is saturated at nearly 28 and never reached 30, this maybe due to the capacity of actor model is too low to capture the policy complexity. However, even without batch normalization and gradient clipping methods, the training curve still seems to be stable and reaching saturated state after 100 episodes. 

![Training score][img_training_result_v1]

**With batch normalization, gradient clipping and eps noise annealing**

Unfortunately, although my models were successfully saved when training on Udacity GPU platform, somehow the output of the training cell did not record the success message and even the recorded training score is lost when the GPU time run out (it costs me 35 hours to train this model!). I am not sure what cause this. However, the mean of training score window is still recorded in `Continuous_Control.ipynb` and I suspect the mean reaches the value of 30 at around episode 150 and saturates at the value of 36 for the rest of the episodes.

#### Testing scores

To evaluate if my agent is succesfully trained for both model settings, this section provides testing score averaged over 100 episodes, of which no learning process happens. 

**Without batch normalization, gradient clipping and eps noise annealing**

Suprisingly, the mean testing score over 100 episodes reaches the value of 37! The reason is that the `eps` noise scale is set to zero in the testing phase (not constant 1.0 as in training phase), the actor model becomes deterministic and the agents track the green balls more stable.

![Testing score without batch normalization][img_testing_result_v1] 

**With batch normalization, gradient clipping and eps noise annealing**

As expect, the mean testing score over 100 episodes reaches the value of 37.8. This is because the training curve is more stable and eps noise annealing method somewhat satisfies the GLIE condition.
 
![Testing score with batch normalization][img_testing_result] 

## 4. Future works

Finally, some of the improvements we can consider making in the following updates to this project:

* Investigate the effects of the network capacity in the training dynamics, as can be seen that the actor and critic models have small networks to gain the stability.

* Investigate the effects of initilization in the training dynamics, as sometime the agent does not learn or capture environment characteristics.

* Implement other Policy Gradient algorithms to solve this task, like PPO and SAC, and compare them
  with our DDPG implementation as baseline.

* Try to solve the optional Walker Task (crawler env. from ml-agents)

## References

* [1] Lillicrap et. al.. [*Continuous control through deep reinforcement learning*](https://arxiv.org/pdf/1509.02971.pdf)
* [2] Silver et. al.. [*Deterministic Policy Gradients Algorithms*](http://proceedings.mlr.press/v32/silver14.pdf)
* [3] Mnih, Volodymyr & Kavukcuoglu, Koray & Silver, David, et. al.. [*Human-level control through deep-reinforcement learning*](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)
* [4] [DDPG implementation from Udacity](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum)
* [5] [Ornsteinâ€“Uhlenbeck process](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process)